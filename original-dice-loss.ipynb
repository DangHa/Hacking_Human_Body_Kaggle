{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2022-09-25T12:51:43.952358Z","iopub.status.busy":"2022-09-25T12:51:43.951986Z","iopub.status.idle":"2022-09-25T12:51:45.875199Z","shell.execute_reply":"2022-09-25T12:51:45.873677Z","shell.execute_reply.started":"2022-09-25T12:51:43.952333Z"},"trusted":true},"outputs":[],"source":["import os\n","from PIL import Image\n","import time\n","import pickle\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.nn import init\n","from torch.utils.data import Dataset, DataLoader\n","import torchvision\n","import numpy as np\n","import math\n","\n","import matplotlib.pyplot as plt\n","import matplotlib as mpl\n","import matplotlib.patches as patches\n","mpl.rcParams['figure.figsize'] = [10, 10]\n","\n","import pandas as pd\n","import warnings\n","warnings.filterwarnings('ignore')"]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"markdown","metadata":{},"source":["## Unet architecture"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2022-09-25T12:51:54.162184Z","iopub.status.busy":"2022-09-25T12:51:54.161782Z","iopub.status.idle":"2022-09-25T12:51:54.197226Z","shell.execute_reply":"2022-09-25T12:51:54.194873Z","shell.execute_reply.started":"2022-09-25T12:51:54.162152Z"},"trusted":true},"outputs":[],"source":["def crop_skip_connection(output, starting_depth, total_depth):\n","\n","  original_shape = output.shape[-1]\n","  final_shape = original_shape\n","  depth = total_depth - starting_depth\n","  # encoder pathway\n","  for i in range(depth):\n","    final_shape /= 2\n","    final_shape -= 4\n","  # decoder pathway\n","  for i in range(depth):\n","    final_shape *= 2\n","    if i != depth-1:\n","      final_shape -= 4\n","  margin = (original_shape - final_shape)/2 \n","  start = int(margin)\n","  end = int(original_shape - margin)\n","  output = output[..., start:end, start:end]\n","  return output\n","\n","\n","# U-net\n","def conv3d(in_channels, out_channels, stride=1, \n","            padding=0, bias=True, groups=1):    \n","    return nn.Conv2d(\n","        in_channels,\n","        out_channels,\n","        kernel_size=3,\n","        stride=stride,\n","        padding=padding,\n","        bias=bias,\n","        groups=groups)\n","\n","def upconv2d(in_channels, out_channels, mode='transpose'):\n","    return nn.ConvTranspose2d(\n","        in_channels,\n","        out_channels,\n","        kernel_size=2,              \n","        stride=2)\n","\n","def conv1d(in_channels, out_channels, groups=1):\n","    return nn.Conv2d(\n","        in_channels,\n","        out_channels,\n","        kernel_size=1,\n","        groups=groups,\n","        stride=1)\n","\n","class Encoder(nn.Module):\n","    def __init__(self, in_channels, out_channels, pooling=True):\n","        super(Encoder, self).__init__()\n","\n","        self.in_channels = in_channels\n","        self.out_channels = out_channels\n","        self.pooling = pooling\n","\n","        self.conv1 = conv3d(self.in_channels, self.out_channels)\n","        ## !TODO: adding batchnorm\n","        self.conv2 = conv3d(self.out_channels, self.out_channels)\n","\n","        if self.pooling:\n","            self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n","\n","    def forward(self, x):\n","        x = F.relu(self.conv1(x))\n","        x = F.relu(self.conv2(x))\n","        before_pool = x  # for skip connection\n","        if self.pooling:\n","            x = self.pool(x)\n","        return x, before_pool\n","\n","class Decoder(nn.Module):\n","    def __init__(self, in_channels, out_channels, \n","                 merge_mode='concat', up_mode='transpose'):\n","        super(Decoder, self).__init__()\n","\n","        self.in_channels = in_channels\n","        self.out_channels = out_channels\n","        self.merge_mode = merge_mode\n","        self.up_mode = up_mode\n","\n","        self.upconv = upconv2d(self.in_channels, self.out_channels, \n","            mode=self.up_mode)\n","\n","        self.conv1 = conv3d(2*self.out_channels, self.out_channels)\n","\n","        self.conv2 = conv3d(self.out_channels, self.out_channels)\n","\n","\n","    def forward(self, from_down, from_up):\n","        from_up = self.upconv(from_up)\n","\n","        if self.merge_mode == 'concat':\n","            x = torch.cat((from_up, from_down), 1)\n","        else:\n","            x = from_up + from_down\n","        x = F.relu(self.conv1(x))\n","        x = F.relu(self.conv2(x))\n","        return x\n","\n","class UNet(nn.Module):\n","\n","    def __init__(self, num_classes, in_channels=3, depth=5, \n","                 start_filts=64, up_mode='transpose', \n","                 merge_mode='concat', init='paper'):\n","        \n","        super(UNet, self).__init__()\n","\n","        self.num_classes = num_classes\n","        self.in_channels = in_channels\n","        self.start_filts = start_filts\n","        self.depth = depth\n","        self.init = init\n","\n","        self.down_convs = []\n","        self.up_convs = []\n","\n","        for i in range(depth):\n","            ins = self.in_channels if i == 0 else outs\n","            outs = self.start_filts*(2**i)\n","            pooling = True if i < depth-1 else False\n","\n","            down_conv = Encoder(ins, outs, pooling=pooling)\n","            self.down_convs.append(down_conv)\n","\n","        for i in range(depth-1):\n","            ins = outs\n","            outs = ins // 2\n","            up_conv = Decoder(ins, outs, up_mode=up_mode,\n","                merge_mode=merge_mode)\n","            self.up_convs.append(up_conv)\n","\n","        self.conv_final = conv1d(outs, self.num_classes)\n","        self.sigmoid = nn.Sigmoid()\n","\n","        self.down_convs = nn.ModuleList(self.down_convs)\n","        self.up_convs = nn.ModuleList(self.up_convs)\n","\n","        self.reset_params(self.init)\n","\n","    @staticmethod\n","    def weight_init(m, mode):\n","        if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d)):\n","            if mode=='paper':\n","                in_nodes = m.kernel_size[0]**2 * m.in_channels\n","                std = np.sqrt(2 / in_nodes)\n","                init.normal(m.weight, mean=0, std=std)\n","            elif mode=='xavier':\n","                init.xavier_normal(m.weight)\n","            init.constant(m.bias, 0)\n","\n","    def reset_params(self, mode):\n","        for i, m in enumerate(self.modules()):\n","            self.weight_init(m, mode)\n","\n","    def forward(self, x):\n","        encoder_outs = []\n","         \n","        for i, module in enumerate(self.down_convs):\n","            x, before_pool = module(x)\n","            cropped = crop_skip_connection(before_pool, i+1, self.depth)\n","            encoder_outs.append(cropped)\n","\n","        for i, module in enumerate(self.up_convs):\n","            before_pool = encoder_outs[-(i+2)]\n","            x = module(before_pool, x)\n","    \n","        x = self.conv_final(x)\n","        x = self.sigmoid(x).view(x.shape[0], x.shape[-2], x.shape[-1])\n","        return x"]},{"cell_type":"markdown","metadata":{},"source":["# Helper Functions"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2022-09-25T12:52:13.560689Z","iopub.status.busy":"2022-09-25T12:52:13.560354Z","iopub.status.idle":"2022-09-25T12:52:13.599673Z","shell.execute_reply":"2022-09-25T12:52:13.598211Z","shell.execute_reply.started":"2022-09-25T12:52:13.560665Z"},"trusted":true},"outputs":[],"source":["### ----- Cleaning data -------\n","def remove_white_area(image):\n","    '''\n","      output is (2560, 2560, 3) for images and (2560, 2560) for labels\n","    '''\n","    size = 2560\n","    ### images\n","    if len(image.shape) == 3:\n","      if image.shape[0] >= size:\n","        x = (image.shape[0]-size)//2\n","        y = image.shape[1]-(image.shape[1]-size)//2\n","\n","        if ((size-image.shape[0])%2 == 1):\n","          return image[x:y-1, x:y-1, :]\n","        return image[x:y, x:y, :]\n","      \n","      ## padding (add more white area)\n","      x = (size-image.shape[0])//2\n","      y = (size-image.shape[1])//2\n","      \n","      if ((size-image.shape[0])%2 == 1):\n","        return np.pad(image, ((x, x+1), (y, y+1), (0, 0)), constant_values=(245, 245))\n","      return np.pad(image, ((x, x), (y, y), (0, 0)), constant_values=(245, 245))\n","      \n","    ### labels\n","    if len(image.shape) == 2:\n","      if image.shape[0] >= size:\n","        x = (image.shape[0]-size)//2\n","        y = image.shape[1]-(image.shape[1]-size)//2\n","\n","        if ((size-image.shape[0])%2 == 1):\n","          return image[x:y-1, x:y-1]\n","        return image[x:y, x:y]\n","\n","      ## padding (add more white area)\n","      x = (size-image.shape[0])//2\n","      y = (size-image.shape[1])//2\n","\n","      if ((size-image.shape[0])%2 == 1):\n","        return np.pad(image, ((x, x+1), (y, y+1)), constant_values=(0, 0))\n","      return np.pad(image, ((x, x), (y, y)), constant_values=(0, 0))\n","\n","####--- Patching for original Unet ------# \n","def input_compatible(output_shape, depth):\n","# Given an output shape of a contracting U-net, calculate the necessary input shape to \n","# produce it.\n","  # Going back through decoder:\n","  input_shape = float(output_shape)\n","  for _ in range(depth-1):\n","    input_shape += 4 # restore borders lost to convolution\n","    input_shape /= 2 # undo Upampling\n","    if not input_shape.is_integer():  \n","      return False \n","  # Going back through encoder:\n","  for i in range(depth):\n","    input_shape += 4 # restore borders lost to convolution\n","    if i == depth-1:\n","      continue\n","    input_shape *= 2 # undo Downsampling\n","    if not input_shape.is_integer():  \n","      return False \n","  return input_shape\n","\n","def Unet_padding(output_shape, depth):\n","# Returns padding necesary for input on each border for a desired output.\n","# Output shape should be a scalar (width or height)\n","  input_shape = input_compatible(output_shape, depth)\n","  padding =  (input_shape - output_shape) / 2\n","  return padding\n","\n","def cut_img(img, label, factor=4, contractingUnet=True, depth=5):\n","  # Img shape should be (3, height, width), each patch will have heigt = original_height / factor \n","  # Label shape should be (height, width)\n","  # so it produces factor^2 patches\n","  # If the image height or width is not divisible by the factor, padding with 0 is performed on each side\n","  # until it is or rows and columns get deleted (preferred).\n","  assert img.shape[0] == 3, f'img shape must be (3,h,w) but was given {img.shape}'\n","  assert label.shape[0] == label.shape[1], f'label shape must be (h,w) but was given {label.shape}'\n","\n","  # Or remove rows and columns from the image\n","  data = torch.cat([img,label[None, :, :]]) # Should have dim (4, height, width) with 4th channel being the labels\n","  height = img.shape[1]\n","  i = 0\n","  while height % factor != 0:\n","    data = data[:, :-1, :-1] if i%2==0 else data[:, 1:, 1:] # Remove alternating between top-left and bottom-right\n","    height = data.shape[1]\n","    i += 1\n","\n","  output_shape = height / factor\n","  img_patches = []\n","  label_patches = []\n","\n","  if contractingUnet:\n","    # Check if tiling is compatible with some Unet input, if not reduce output shape\n","    while not input_compatible(output_shape, depth):\n","      output_shape -= 1\n","    # Reduce image to adapt to new tiling shape\n","    new_data_shape = output_shape * factor\n","    i = 0\n","    while data.shape[-1] != new_data_shape:\n","      data = data[:, :-1, :-1] if i%2==0 else data[:, 1:, 1:] # Remove alternating between top-left and bottom-right\n","      i += 1\n","    # Separate data and labels again, transformation for input_shape is only performed on image data\n","    labels = data[3]\n","    img = data[:3]\n","    # Calculate the padding needed for input to Unet\n","    padding =  Unet_padding(output_shape, depth) \n","    # Cut the image\n","    for i in range(factor):\n","      for j in range(factor):\n","        # Image patch \n","        # Location in image:\n","        h = i * output_shape\n","        w = j * output_shape\n","        padding = int(padding)\n","        # Check if image needs reflection\n","        top_ref = True if i == 0 else False\n","        bot_ref = True if i == factor - 1 else False\n","        left_ref = True if j == 0 else False\n","        right_ref = True if j == factor - 1 else False\n","        # Cases\n","        hl = int(np.maximum(h - padding, 0))\n","        hr = int(h + padding + output_shape)\n","        wl = int(np.maximum(w - padding, 0))\n","        wr = int(w + padding + output_shape)\n","        patch = img[:, hl:hr, wl:wr]\n","        if left_ref:\n","          padder = nn.ReflectionPad2d((padding,0,0,0))\n","          patch = padder(patch)\n","        if right_ref:\n","          padder = nn.ReflectionPad2d((0,padding,0,0))\n","          patch = padder(patch)\n","        if top_ref:\n","          padder = nn.ReflectionPad2d((0,0,padding,0))\n","          patch = padder(patch)\n","        if bot_ref:\n","          padder = nn.ReflectionPad2d((0,0,0,padding))\n","          patch = padder(patch)\n","        img_patches.append(patch)\n","      # Label patch\n","        hl = int(h)\n","        hr = int(h + output_shape)\n","        wl = int(w)\n","        wr = int(w + output_shape)\n","        label_patch = labels[hl:hr, wl:wr]\n","        label_patches.append(label_patch)\n","  else:\n","    for i in range(factor):\n","      for j in range(factor):\n","        hl = int(i * output_shape)\n","        wl = int(j * output_shape)\n","        hr = int(hl + output_shape)\n","        wr = int(wl + output_shape)\n","        patch = data[:, hl:hr, wl:wr]\n","        patches.append(patch)\n","  img_patches = torch.stack(img_patches)\n","  label_patches = torch.stack(label_patches)\n","  return img_patches, label_patches\n","\n","\n","### ----- Image decoding ----\n","def rle_decode(mask_rle, shape):\n","    '''\n","    mask_rle: run-length as string formated (start length)\n","    shape: (height,width) of array to return \n","    Returns numpy array, 1 - mask, 0 - background\n","    '''\n","    s = mask_rle.split()\n","    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n","    starts -= 1\n","    ends = starts + lengths\n","    img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n","    for lo, hi in zip(starts, ends):\n","        img[lo:hi] = 1\n","    return img.reshape(shape)\n","\n","def image_recon(patches):\n","  # patches is a tensor of shape (#ofpatches, patch_size, patch_size)\n","  # Assumes #ofpatches is a perfect square\n","  shape = patches.shape\n","  per_row = int(math.sqrt(shape[0]))\n","  patch_size = shape[1]\n","  recon = patches.contiguous().view(per_row, per_row, patch_size, patch_size)\n","  recon = recon.permute(0,2,1,3).reshape(per_row*patch_size, per_row*patch_size)\n","  return recon\n","\n","def unnormalize(img):\n","  # expected img dim: (3, height, width)\n","  # Returns (3, height, width) unnormalized\n","  mean_tensor = torch.tensor([0.8291, 0.8037, 0.8217])\n","  std_tensor = torch.tensor([0.1692, 0.1958, 0.1842])\n","  assert img.shape[0] == 3, f\"Img shape should be (3, height, width), got {img.shape}\"\n","  img = img * std_tensor[:, None, None] + mean_tensor[:, None, None]\n","  return img\n","\n","# Dice coefficient\n","\n","def dice_coeff(pred, target):\n","  overlap = torch.mul(pred, target).sum()\n","  area = target.sum() + pred.sum()\n","  if area == 0:\n","    return 1\n","  return 2 * overlap / area\n","\n","def dice_loss(pred, target):\n","  overlap = torch.mul(pred, target).sum()\n","  area = target.sum() + pred.sum()\n","  epsilon = 0.001 # For cases where area = 0\n","  dice = (2 * overlap + epsilon) / (area + epsilon)\n","  return 1 - dice\n","\n","# Utility for gpu \n","def cuda_mem(x='..'): # Function to check memory in gpu \n","  print(f'{torch.cuda.memory_allocated() * 9.3e-10 :.2f} Gb used after: {x}')\n","\n","######------ Precomputed Parameters----\n","# Mean and Std across whole dataset for original data\n","# mean_tensor = torch.tensor([0.8291, 0.8037, 0.8217])\n","# std_tensor = torch.tensor([0.1692, 0.1958, 0.1842])\n","\n","# Mean and Std across whole dataset for original augmented data\n","mean_tensor = torch.tensor([0.8142, 0.7921, 0.8040])\n","std_tensor = torch.tensor([0.1712, 0.1966, 0.1855])\n","\n","# Transformation\n","transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor(),\n","                                            torchvision.transforms.Normalize(\n","                                              mean = mean_tensor,\n","                                              std = std_tensor)])"]},{"cell_type":"markdown","metadata":{},"source":["## Data set"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2022-09-25T12:52:13.639658Z","iopub.status.busy":"2022-09-25T12:52:13.639290Z","iopub.status.idle":"2022-09-25T12:52:25.712841Z","shell.execute_reply":"2022-09-25T12:52:25.711124Z","shell.execute_reply.started":"2022-09-25T12:52:13.639626Z"},"trusted":true},"outputs":[],"source":["class KaggleDataset(Dataset):\n","    def __init__(self, image_dir, label_dir):\n","        self.image_dir = image_dir\n","        self.labels = pd.read_csv(label_dir)\n","\n","    def __len__(self):\n","        return len(self.labels)\n","\n","    def __getitem__(self, index):\n","        # Labels\n","        labels = self.labels.iloc[index]\n","        id = str(labels['id'])\n","        # Image\n","        img_path = os.path.join(self.image_dir, id + \".jpeg\")\n","        image = np.array(Image.open(img_path).convert(\"RGB\"))\n","        # rle\n","        rle = labels['rle']\n","        decoded_rle = rle_decode(rle, shape = image.shape[:2])\n","\n","        return transform(remove_white_area(image)).permute(1,2,0), torch.tensor(remove_white_area(decoded_rle))\n","\n","class PatchDS(Dataset):\n","    # Class used in training loop to load the patches in a batch \n","    def __init__(self, images, labels):\n","        self.images= images\n","        self.labels = labels\n","\n","    def __len__(self):\n","        return self.images.shape[0]\n","\n","    def __getitem__(self, index):\n","      return self.images[index], self.labels[index]\n","\n","IMAGES_DIRECTORY = \"/home/daniel/Proyectos/Kaggle-HuBMAP-HPA/augmented_data/images/\"\n","TRAIN_LABEL_DIRECTORY = \"/home/daniel/Proyectos/Kaggle-HuBMAP-HPA/augmented_data/augmented_labels.csv\"\n","\n","dataset = KaggleDataset(\n","    image_dir=IMAGES_DIRECTORY,\n","    label_dir=TRAIN_LABEL_DIRECTORY,\n",")"]},{"cell_type":"markdown","metadata":{},"source":["# Metamodel"]},{"cell_type":"code","execution_count":6,"metadata":{"trusted":true},"outputs":[],"source":["class metamodel():\n","  def __init__(self, model, depth):\n","    self.model = model\n","    self.depth = depth\n","\n","  def predict(self, image, label):\n","    image = image.permute(2,0,1)\n","    image_patches, label_patches = cut_img(image, label, depth=self.depth)\n","    img_pred = []\n","    for i, patch in enumerate(DataLoader(image_patches)):\n","      patch = patch.float().to(device)\n","      pred = model(patch).squeeze()\n","      pred = torch.where(pred>=0.5, 1, 0)\n","      pred = pred.detach()\n","      img_pred.append(pred)\n","    img_pred = torch.stack(img_pred)\n","    img_pred = image_recon(img_pred)\n","    label = [label_patches[i] for i in range(label_patches.shape[0])]\n","    label = torch.stack(label)\n","    label = image_recon(label).to(device)\n","    return img_pred.type(torch.long), label.type(torch.long)"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["model = UNet(num_classes=1, depth=4, merge_mode='concat').to(device)\n","meta = metamodel(model, 4)\n","img, lab = dataset[0]\n","pred,lab = meta.predict(img, lab)"]},{"cell_type":"markdown","metadata":{},"source":["# Training"]},{"cell_type":"code","execution_count":18,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["0.00 Gb used after: Starting epoch 0\n","tensor(0.4585, grad_fn=<RsubBackward1>)\n","tensor(0.4585, grad_fn=<RsubBackward1>)\n"]}],"source":["# Training the Unet\n","\n","# Train / Test split\n","total_elements = np.arange(len(dataset))\n","originals = [i for i in total_elements if i%3==0]\n","test_elements = np.random.choice(originals, 15)\n","removed = [[i, i+1, i+2] for i in test_elements]\n","removed = [item for sublst in removed for item in sublst]\n","total_elements = np.delete(total_elements, removed)\n","train_elements = total_elements\n","np.random.shuffle(train_elements)\n","\n","# Training parames\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","weight_tensor = torch.tensor([0.1325, 0.8675])\n","learning_rate = 0.001\n","depth = 6\n","number_of_epoch = 3\n","factor = 4 # Factor to divide the image: factor -> factor**2 patches per image\n","patch_batchsize = 2\n","\n","model = UNet(num_classes=1, depth=depth, merge_mode='concat').to(device)\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","\n","last_elements = int( 50 * factor / patch_batchsize) # How many elements to average over to get mean loss every 50 imgs\n","loss_plot = []\n","epoch_plot = []\n","loss_in_epoch = {i:[] for i in range(number_of_epoch)} # To save loss within epoch, every 50 images\n","dice_individual = {i:[] for i in range(number_of_epoch)}\n","dice_avg = []\n","\n","for epoch in range(number_of_epoch):\n","    cuda_mem(f\"Starting epoch {epoch}\")\n","    np.random.shuffle(train_elements)\n","    loss_epoch = []\n","    for img_counter, element_index in enumerate(train_elements):\n","        image , label = dataset[element_index]\n","        image = image.permute(2,0,1)\n","        image_patches, label_patches = cut_img(image, label, depth=depth)\n","        patch_dataset = PatchDS(image_patches, label_patches)\n","        patch_loader = DataLoader(patch_dataset, batch_size=patch_batchsize, shuffle=True)\n","        for patch_counter, (patches, labels) in enumerate(patch_loader):\n","            # Forward\n","            outputs = model(patches.float().to(device))\n","            labels = labels.long().to(device)\n","            # Backwards\n","            optimizer.zero_grad()\n","            loss = dice_loss(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","            # Gather data\n","            loss_epoch.append(loss.item())\n","        if img_counter % 50 == 0:\n","            if img_counter == 0:\n","                avg_loss = loss.item()\n","            else:\n","                avg_loss = np.array(loss_epoch[-last_elements:]).mean()\n","            loss_in_epoch[epoch].append(avg_loss)\n","            print(f'Epoch {epoch}: image: {img_counter+1} / {len(train_elements)}, loss (last 50) = {avg_loss :.2f}')                 \n","    loss_epoch = np.array(loss_epoch)\n","    loss_plot.append(sum(loss_epoch) / len(loss_epoch))\n","    epoch_plot.append(epoch)\n","    # Calculate dice coefficient\n","    dice_avg_aux = []\n","    for test_idx in test_elements:\n","        img, label = dataset[test_idx]\n","        meta = metamodel(model, depth)\n","        pred, target = meta.predict(img, label)\n","        dice = dice_coeff(pred, target).cpu()\n","        dice_avg_aux.append(dice)\n","        dice_individual[epoch].append(dice)\n","    dice_avg.append(np.array(dice_avg_aux).mean())\n","    del pred\n","    del target\n","    print(f'Epoch:{epoch}:  Dice coeff {dice_avg[epoch]}')\n","    print(f\"{'-'*20}Finished Epoch {epoch} {'-'*20}\")"]},{"cell_type":"markdown","metadata":{},"source":["# Saving"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Save trained model\n","torch.save(model, 'model.pth')\n","# Save epoch loss\n","np.savetxt(\"./loss_plot.csv\", loss_plot, delimiter=\",\")\n","np.savetxt(\"./epoch_plot.csv\", epoch_plot, delimiter=\",\")\n","# Save within epoch loss\n","with open('./in_epoch_loss_dic.pkl', 'wb') as f:\n","    pickle.dump(loss_in_epoch, f)\n","# Save Dice coeff\n","np.savetxt(\"./dice_coeff.csv\", dice_avg, delimiter=\",\")\n","with open('./dice_individual.pkl', 'wb') as f:\n","    pickle.dump(dice_individual, f)"]}],"metadata":{"kernelspec":{"display_name":"Python 3.8.10 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"},"vscode":{"interpreter":{"hash":"916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"}}},"nbformat":4,"nbformat_minor":4}
